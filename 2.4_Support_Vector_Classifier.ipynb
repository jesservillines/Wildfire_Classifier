{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPAV7zw1dszh"
   },
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Imports](#Imports)\n",
    "- [Prepare Dataset](#Prepare-Dataset)\n",
    "- [OPTION: Cluster Latitude and Longitude data](#OPTION:-Cluster-Latitude-and-Longitude-data)\n",
    "- [Bootstrap](#Bootstrap)\n",
    "- [One Hot Encode Features](#One-Hot-Encode-Features)\n",
    "- [Create Scaled X, y and Train, Test](#Create-Scaled-X,-y-and-Train,-Test)\n",
    "- [Compile and Fit Support Vector Classifier](#Compile-and-Fit-Support-Vector-Classifier)\n",
    "- [Model Evaluation](#Model-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook explores the Support Vector Classifier. This model was considered for feature importance, but took more resources to run than the Random Forest Classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPeI3XU1dszo"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score, accuracy_score, f1_score, make_scorer, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in cleaned and combined data from AWS \n",
    "noaa_on_fire = pd.read_csv('s3://git-to-amazon-s3-outputbucket-rorni8oehk4l/soulclimberchick/meteorology-fire-impact/data-files/mfi_df_yr_trail.csv')\n",
    "noaa_on_fire.drop(columns='Unnamed: 0', inplace=True)\n",
    "noaa_on_fire.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_on_fire_btstrp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION: Cluster Latitude and Longitude data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Put lat/long into a matrix\n",
    "# location_data = round(noaa_on_fire[['longitude', 'latitude']],4) # round to decrease grid accuracy of lat/long\n",
    "\n",
    "# # Cluster lat/long\n",
    "# km = KMeans(n_clusters=200)\n",
    "# km.fit(location_data)\n",
    "\n",
    "# # Append clusters back into model_df\n",
    "# noaa_on_fire.loc[:, 'cluster'] = km.predict(location_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ol-vz-ddszt"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire_btstrp = pd.concat([noaa_on_fire, \n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'C'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'D'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'E'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'F'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'G'].sample(n = 100000, replace = True, random_state=11)], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_on_fire_btstrp = pd.get_dummies(noaa_on_fire, columns=['state', 'month'], drop_first= True)\n",
    "#noaa_on_fire = pd.get_dummies(noaa_on_fire, columns=['cluster']) # location clustered by lat/long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scaled X, y and Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y variables, and bootstrap data\n",
    "\n",
    "X = noaa_on_fire_btstrp[[ 'pcp', 'tavg', 'pdsi', 'phdi', 'zndx', 'pmdi', 'sp02','sp03', 'sp06', 'sp09', #10\n",
    "                         'sp12', 'sp24', 'tmin', 'tmax', 'month_2', 'month_3','month_4', 'month_5', 'cdd', 'hdd', #20\n",
    "                         'month_6', 'month_7', 'month_8', 'month_9','month_10', 'month_11', 'month_12', 'tavg_t12m','tavg_t9m', 'tavg_t6m', #30\n",
    "                         'tavg_t3m', 'pcp_t12m', 'pcp_t9m', 'pcp_t6m','pcp_t3m', 'pmdi_t12m', 'pmdi_t9m', 'pmdi_t6m', 'pmdi_t3m', 'pdsi_t12m', #40\n",
    "                         'pdsi_t9m', 'pdsi_t6m', 'pdsi_t3m']] #43\n",
    "\n",
    "y = noaa_on_fire_btstrp['fire_size_class']\n",
    "\n",
    "# Train Test Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "#Standard Scale data\n",
    "stan = StandardScaler()\n",
    "X_train = stan.fit_transform(X_train)\n",
    "X_test = stan.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the accuracy of our baseline model?\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Fit Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqZeCBrUJpqz"
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Support Vector Classifier achieves accuracy of {round(svc.score(X_train, y_train),4)} on train data and {round(svc.score(X_test, y_test),4)} on test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPyBoaNIKAm5"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc, X_test, y_test,cmap = 'YlOrBr', normalize = 'true');"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jRUpNQ_sdszw",
    "ejWbHtqSdszx",
    "By0ndladdszy"
   ],
   "machine_shape": "hm",
   "name": "models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
